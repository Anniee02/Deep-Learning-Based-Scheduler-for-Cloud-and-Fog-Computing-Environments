import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch as T
import random
#from util import plot_learning_curve
class LinearDeepQNetwork(nn.Module):
def __init__(self, lr, n_actions, input_dims):
super(LinearDeepQNetwork, self).__init__()
self.fc1 = nn.Linear(input_dims, 128)
self.fc2 = nn.Linear(128, n_actions)
self.optimizer = optim.Adam(self.parameters(), lr=lr)
self.loss = nn.MSELoss()
self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')
self.to(self.device)
#takes the current and returns list of actions
def forward(self, state):
layer1 = F.relu(self.fc1(state))
actions = self.fc2(layer1)
return actions
class Agent():
def __init__(self, input_dims, n_actions, lr, gamma=0.99,
epsilon=1.0, eps_dec=1e-5, eps_min=0.01):
self.lr = lr
self.input_dims = input_dims
self.n_actions = n_actions
self.gamma = gamma
self.epsilon = epsilon
self.eps_dec = eps_dec
self.eps_min = eps_min
self.action_space = [i for i in range(self.n_actions)]
self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)
def choose_action(self, state):
if np.random.random() > self.epsilon:
state1 = T.tensor(state, dtype=T.float).to(self.Q.device)
# state =
actions = self.Q.forward(state1)
action = T.argmax(actions).item()
else:
action = np.random.choice(self.action_space)
return action
def decrement_epsilon(self):
self.epsilon = self.epsilon - self.eps_dec \
if self.epsilon > self.eps_min else self.eps_min
def learn(self, state, action, reward, state_):
self.Q.optimizer.zero_grad()
states = T.tensor(state, dtype=T.float).to(self.Q.device)
actions = T.tensor(action).to(self.Q.device)
rewards = T.tensor(reward).to(self.Q.device)
states_ = T.tensor(state_, dtype=T.float).to(self.Q.device)
q_pred = self.Q.forward(states)[actions]
q_next = self.Q.forward(states_).max()
q_target = reward + self.gamma*q_next
loss = self.Q.loss(q_target, q_pred).to(self.Q.device)
loss.backward()
self.Q.optimizer.step()
self.decrement_epsilon()
def processDQN_stage1(self, initial_state):
action = self.choose_action(initial_state)
return action
def processDQN_stage2(self,initial_state ):
action = self.choose_action(initial_state)
return action
#TESTING MODEL FOR ACTION AND ENVIRONMENT
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch as T
import random
import time
class Task(object):
"""
information of each task
parent, child base on dependency
jobID, index, CPU, RAM, disk extracted from user data
status indicates the current status of the task
"""
def __init__(self, jobID, index, CPU, RAM, disk, status):
import random
import time
self.parent = []
self.child = []
self.jobID = jobID
self.index = index
self.CPU = CPU
self.RAM = RAM
self.disk = disk
self.status = status #-1: rejected, 0: finished, 1: ready, 2: running
self.runtime = random.randint(1, 10)/1000.0
self.ddl = time.time() + self.runtime + random.randint(1, 1000) * 100
self.endtime = 0
class DAG(object):
"""
Transform job queue to task ready queue
"""
def __init__(self, fname, num_task):
self.fname = fname
self.num_task = num_task
self.job = []
self.task = []
def readfile(self):
"""
Read the input job file
All task are initialized to ready status
"""
num_task = 0
with open(self.fname, 'r') as f:
task = []
for line in f:
if line[0] == 'J':
if len(task) != 0:
self.job.append(task)
task = []
else:
info = list(line.strip(' ').split())
task.append(Task(info[1], info[2], float(info[4]), float(info[5]), info[6], 1))
num_task += 1
if num_task == self.num_task:
break
if len(task) != 0:
self.job.append(task)
def checkRing(self, parent, child):
"""
Check whether there is a loop between parent and child
Return True if has loop
"""
if parent.index == child.index:
return True
if len(child.child) == 0:
return False
for c in child.child:
if self.checkRing(parent, c):
return True
return False
def buildDAG(self):
"""
Randomly build dependencies between tasks within each job
"""
import random
for job in self.job:
for task in job:
i = random.randint(-len(job), len(job) - 1)
if i < 0:
continue
parent = job[i]
if self.checkRing(parent, task) == False:
task.parent.append(parent)
parent.child.append(task)
def rejTask(self, task):
"""
If one task is rejected
Then all tasks that depended on this task will be rejected
"""
task.status = -1
for c in task.child:
self.rejTask(c)
def hasParent(self, task):
"""
When a task are finished
Remove it from the parent for all child tasks
"""
for c in task.parent:
if c.status == 1: #still has parent
return True
return False
def updateStatus(self, task):
"""
Given jobid and taskid, change status of all tasks that depend on it
If the task with "-1" status, reject this tasks' all child tasks
If the task with "0" status, remove it from all child tasks
"""
# job_i, task_i = self.findTask(task.jobID, task.index)
# if job_i == -1 or task_i == -1:
# print("WRONG: The task with jobID: ", task.jobID, " and taskID: ", task.index, " not exist.")
# return
# job = self.job[job_i]
# task = job[task_i]
if task.status == -1:
self.rejTask(task)
# elif task.status == 0:
# self.rmParent(task, task_i, job)
def initTask(self):
"""
run readfile and buildDAG functions
"""
self.readfile()
self.buildDAG()
def taskQueue(self):
"""
Build the task ready queue
Just put the one whose status is 1
and whose parent are all finished
"""
for job in self.job:
# num_task = len(job)
# while num_task > 0:
for task in job:
if task.status == 1 and self.hasParent(task) == False:
self.task.append(task)
# task.status = 0
# self.updateStatus(task)
# num_task -= 1
# for t in self.task:
# t.status = 1
# print(len(self.task), "requests")
# self.printTask()
def printTask(self):
"""
Print tasks which are in task queue info
"""
for j in self.task:
print(j.jobID, ",", j.index, ",", j.status, ",", len(j.parent))
class environment(object):
"""docstring for environment
the environment of RP/TS processor
read the task from txt file
calculate the Reward Function
interface with DQN and baseline
"""
def __init__(self, scale, fname, num_task, num_server):
"""
initial the variable
We assume each server has 10 VM
For small-scale problems:
200 servers
10 server farms
For large-scale problems:
4000 servers
70 server farms
All servers have unit CPU, RAM, and Disk space
"""
self.scale = scale
self.fname = fname
self.task = []
self.dag = DAG(self.fname, num_task)
self.VMNum = 5
self.rej = 0
self.num_task = num_task
self.severNum = num_server
if self.scale == 'small':
# self.severNum = 200
self.farmNum = 10
elif self.scale == 'large':
# self.severNum = 4000
self.farmNum = int(self.severNum / 50)
# self.init_severs()
self.remainFarm = []
self.FarmResources = []
self.severs = [[1,1]for _ in range(self.severNum)]
self.VMtask = []
self.totalcost = 0
# print("Total Number of tasks: {0}".format(num_task))
def init_severs(self, severNum):
"""
Set the initial values for each VMs
Each VM has 1/n unit CPU and RAM
Each VM has a task list
"""
VM = [[[1.0/self.VMNum, 1.0/self.VMNum]for _ in range(self.VMNum)]for _ in range(severNum)]
# VM = [[[1.0 , 1.0 ] for _ in range(self.VMNum)] for _ in range(severNum)]
self.VMtask.append([[[]for _ in range(self.VMNum)]for _ in range(severNum)])
return VM
def generateQueue(self):
self.dag.taskQueue()
self.task = self.dag.task
def setFarm(self):
"""
Randomly set the servers to each farm
Each farm has at least 1 server and at most 2*m/n-1 servers
Initial power usage for each servers and each farm
"""
import random
self.farmOri = []
m = self.severNum
n = self.farmNum
f = int(self.severNum / self.farmNum)
for _ in range(self.farmNum):
# f = random.randint(0,int(2*m/n))
# f = random.randint(1, int(2 * m / n))
self.remainFarm.append(self.init_severs(f))
self.FarmResources.append([f, f])
self.farmOri.append(f)
m -= f
n -= 1
self.farmOri.append(m)
self.pwrPre = [0]*self.severNum #power usage pre sever
self.pwrPFarm = [0]*self.farmNum #power usage per farm
def elecPrice(self, t, pwr):
"""
The energy cost on time t
threshold get from "Impact of dynamic energy pricing schemes on a novel multi-user home energy management system"
price get from "Optimal residential load
control with price prediction in real-time electricity pricing environments"
"""
threshold = 1.5
if pwr < threshold:
p = 5.91 #dynamic price
else:
p = 8.27
return pwr * p
def getPwr(self, r, c):
"""
Implement the energy consumption model
r: the remain CPU
c: the total(unit) CPU
The parameters' value get from "An energy and deadline aware resource provisioning, scheduling and optimization framework for cloud systems"
"""
# eq.2
if r < c:
pwrS = 1
else:
pwrS = 0
alpha = 0.5 #alpha
beta = 10 #beta
Ur = (c-r)/c # eq.1
if Ur < 0.7:
pwrDy = alpha * Ur
else:
pwrDy = 0.7 * alpha + (Ur - 0.7)**2 * beta
return pwrDy+pwrS
def rewardFcn1(self):
"""
Implement the reward function for each farm
For stage 1: choose the farm
"""
# eq.5
pwrCFarm = []
for i in range(self.farmNum):
pwrc = self.getPwr(self.FarmResources[i][0], self.farmOri[i])
pwrCFarm.append(pwrc)
pwr = sum(pwrCFarm) - sum(self.pwrPFarm)
self.pwrPFarm = pwrCFarm
return self.elecPrice(1, pwr)
# def EnergyFun(self):
# """
# Implement the reward function for each server
# For stage 2: choose the server
# """
# # eq.6
# self.totalCost += self.rewardFcn2()
# print ("energy cost: ", self.totalCost)
def rewardFcn2(self):
"""
Implement the reward function for each server
For stage 2: choose the server
"""
# eq.6
pwrCur = []
for f in self.remainFarm:
for s in f:
sremain = 0
for v in s:
sremain += v[0]
# print(sremain)
pwrc = self.getPwr(sremain, 1.0)
if pwrc < 0:
print("here", sremain)
pwrCur.append(pwrc)
# print("pwrc", pwrc)
pwr = sum(pwrCur) - sum(self.pwrPre)
self.totalcost += sum(pwrCur)
# print("sum(pwrCur)", sum(pwrCur), "sum(self.pwrPre)", sum(self.pwrPre))
self.pwrPre = pwrCur
# print("pwr",pwr)
# print("r2", self.elecPrice(1,pwr))
return self.elecPrice(1, pwr)
def release(self):
"""
Randomly release resources from each VM
And set the corresponding task as finished
"""
ranFarm = random.randint(0, self.farmNum-1)
ranSer = random.randint(0, self.farmOri[ranFarm]-1)
ranVM = random.randint(0, self.VMNum-1)
if self.VMtask[ranFarm][ranSer][ranVM]:
random.shuffle(self.VMtask[ranFarm][ranSer][ranVM])
t = self.VMtask[ranFarm][ranSer][ranVM].pop()
t.status = 0
self.remainFarm[ranFarm][ranSer][ranVM][0] += float(t.CPU)
self.remainFarm[ranFarm][ranSer][ranVM][1] += float(t.RAM)
def releaseByTime(self, farm_i, server_i, vm_j):
curtime = time.time()
for t in self.VMtask[farm_i][server_i][vm_j]:
if t.endtime < curtime:
t.status = 0
self.remainFarm[farm_i][server_i][vm_j][0] += float(t.CPU)
self.remainFarm[farm_i][server_i][vm_j][1] += float(t.RAM)
self.FarmResources[farm_i][0] += float(t.CPU)
self.FarmResources[farm_i][1] += float(t.RAM)
self.VMtask[farm_i][server_i][vm_j].remove(t)
def training(self):
"""
Run the DQN/baseline in the RP/TS processor environment
Read the task file by readfile()
Set the variables
Pass tasks to agents in real time
Get the corresponding reward value
Reject task when R_cpu ≥ C_cpu or R_ram < C_ram
"""
#send one tesk to dqn and calculate reward
self.dag.initTask()
self.generateQueue()
time_start=time.time()
print(self.farmNum, end=' ')
print(self.severNum, end=' ')
print(self.num_task, end=' ')
self.trainDQN_v1()
time_end=time.time()
timecost = round(time_end-time_start, 3)
print(timecost, end=' ')
print(round(self.totalcost, 3), end=' ')
print()
def checkRej(self, farm_i, server_i, vm_j, task):
"""
Check whether this task should be rejected in ith sever, jth VM
Reject task when current time + task's runtime > task's ddl
"""
import time
if task.CPU > 1/self.VMNum or task.RAM > 1/self.VMNum:
self.rej += 1
return -1
remain_cpu = self.remainFarm[farm_i][server_i][vm_j][0] - float(task.CPU)
remain_ram = self.remainFarm[farm_i][server_i][vm_j][1] - float(task.RAM)
curtime = time.time()
if curtime + task.runtime <= task.ddl:
if remain_cpu >= 0 and remain_ram >=0:
return 0 # do not reject
else:
return 1 # reject temporarily because cpu or ram
else:
self.rej += 1
return -1 #reject because ddl
def UpdateServerState(self, tempServerFarm, tempSever, vm_numb, task):
self.remainFarm[tempServerFarm][tempSever][vm_numb][0] -= float(task.CPU)
self.remainFarm[tempServerFarm][tempSever][vm_numb][1] -= float(task.RAM)
self.FarmResources[tempServerFarm][0] -= float(task.CPU)
self.FarmResources[tempServerFarm][1] -= float(task.RAM)
return self.custom_reshape(self.remainFarm)
def custom_reshape(self, a):
result = []
for farNum in a:
for serNum in farNum:
result.append(serNum)
c = np.array(result)
d = c.reshape(2 * self.VMNum * self.severNum)
return d
"""
# initialize the DQN stage 2 and environment for it, DQN(intial_state, Num_actions)
# state = state of all VMs in a server farm
# numb_actions = num of VMs in that sever
# initiate DQN stage 1 and env for it, state will be only server remainFarm
# reconstruct self.remainFarm with 3 Dimensional array with heiracrchy as
# farmNumb->serverNumb->vmNumb[cpu, memory, local] eg. [[[0.002, 0.005, 0.003], [],],[],]
# convert the remainFarm into 1D array
# fun to upadate server state after the task has been assigned to server farm
"""
def trainDQN_v1(self):
rej = 0
self.setFarm()
energy = 0
input_stage2 = input_stage1 = self.custom_reshape(self.remainFarm)
Agent_stage1 = Agent(lr=0.0001, input_dims=len(input_stage1),
n_actions=self.farmNum)
stage1_current_state = input_stage1
# input_stage2 = np.array(self.remainFarm[0]).reshape(2*self.VMNum*int(self.severNum/self.farmNum))
Agent_stage2 = Agent(lr=0.0001, input_dims=len(input_stage2),
n_actions=int(self.severNum/self.farmNum))
stage2_current_state = input_stage2
acc = 0
while len(self.task) != 0:
# print(len(self.task))
while len(self.task) != 0:
for t in self.task:
if t.status == -1: #rejected
self.dag.updateStatus(t)
self.task.remove(t)
elif t.status == 1: #ready
f = stage1_action = Agent_stage1.processDQN_stage1(stage1_current_state)
s = stage2_action = Agent_stage2.processDQN_stage2(stage2_current_state)
vm = random.randint(0,self.VMNum-1)
self.releaseByTime(f, s, vm) # release by time
rej = self.checkRej(f, s, vm, t)
if rej == -1: #rejected due to ddl
t.status = -1
# if not reject:
elif rej == 0:
t.endtime = time.time() + t.runtime
stage1_next_state = stage2_next_state = self.UpdateServerState(f, s, vm, t)
# print(self.remainFarm)
reward_stage2 = self.rewardFcn2()
energy += reward_stage2
Agent_stage2.learn(stage2_current_state, stage2_action, reward_stage2, stage2_next_state)
stage2_current_state = stage2_next_state
reward_stage1 = self.rewardFcn1()
Agent_stage1.learn(stage1_current_state, stage1_action, reward_stage1, stage1_next_state)
stage1_current_state = stage1_next_state
self.VMtask[f][s][vm].append(t)
t.status = 2
# self.dag.updateStatus(t)
self.task.remove(t)
acc += 1
# else:
# t.status = -1
# rej += 1
self.generateQueue()
# print("total number of tasks: {0}, rejected tasks: {1}".format(len(self.task), rej))
print(round(1 - acc/self.num_task, 3), end= ' ')
p1 = environment('small', 'input.txt')
p1.training()
